# K-means クラスタリング実装

## 概要

このノートブックは、k-meansアルゴリズムを用いたクラスタリングの実装と可視化を行います。k-meansは教師なし学習における代表的なクラスタリング手法の1つです。

## k-meansアルゴリズムについて

### アルゴリズムの流れ

1. ランダムに初期クラスタの中心を選択
2. 各データポイントを最も近いクラスタ中心に割り当て
3. 各クラスタの中心を再計算(そのクラスタに属するデータの平均値)
4. 収束するまで2-3を繰り返す

## ノートブックの構成

### 1. ライブラリのインポート

- `numpy`: 数値計算
- `matplotlib`: データの可視化
- `sklearn.datasets.make_blobs`: サンプルデータの生成

### 2. k-means関数の定義

[`kmeans`](kmeans.ipynb)関数を実装しています:

- 初期クラスタ中心をランダムに選択
- 各サンプルと中心との距離を計算
- クラスタ中心を更新
- 収束判定を行い、収束するまで繰り返す

### 3. サンプルデータの生成

`make_blobs`を使用して以下のデータを生成:
- サンプル数: 300
- 特徴量: 2次元
- クラスタ数: 6

### 4. 反復実行の可視化

イテレーション1〜5までのクラスタリング結果を可視化し、アルゴリズムの収束過程を確認します。

### 5. クラスタ数の比較

異なるクラスタ数(2, 4, 5, 6, 7, 8)でのクラスタリング結果を比較します。

### 考察

- 今回のデータでは、4〜6個のクラスタが適切であると考えられる
- 実際のデータでは最適なクラスタ数を事前に知ることは困難
- より高度な手法として、EM法などが考えられる

## 実行方法

```bash
jupyter notebook kmeans.ipynb
```

## 必要なライブラリ

```bash
pip install numpy matplotlib scikit-learn
```

## 注意点

- k-meansは初期値に依存するため、結果が毎回異なる可能性があります
- 適切なクラスタ数の選択が重要です
- 球状のクラスタに対して効果的ですが、複雑な形状には不向きな場合があります